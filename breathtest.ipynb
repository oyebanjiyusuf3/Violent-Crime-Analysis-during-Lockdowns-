{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6c6184",
   "metadata": {},
   "source": [
    "# STUDENT NAME: YUSUF OYEBANJI\n",
    "\n",
    "\n",
    "# STUDENT ID: W20027750 \n",
    "\n",
    "\n",
    "# COURSE CODE: KF7032"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1a35a",
   "metadata": {},
   "source": [
    "I imported the pyspark to be able to use Spark Context. Pyspark is an interface for Apache Spark in python programming. The Spark is application programming interface (API) in Python that allows us to write Spark Application. After importing pyspark, I imported SparkContext and SparkConf from pyspark. The SparkContext represents the connection to the spark clusters which is used to create Resilient Distributed Dataset (RDD)  and broadcast variables on that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf=SparkConf().setMaster('local').setAppName('breathest');\n",
    "spark_context=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb1233",
   "metadata": {},
   "source": [
    "I imported SparkSession in other to create a SparkSession in my program in PySpark, I need to use the builder pattern method builder() which is explained below. getOrCreate() method returns an already existing SparkSession; if it does not exist, it creates a new SparkSession and the appName() is used to set my application name.\n",
    "\n",
    "The pyspark.sql.types class is a base class of all types in PySpark which defined in a package pyspark.sql.types.* and they are used to create DataFrame with a specific type.\n",
    "\n",
    "The PySpark has a built-in standard Aggregate functions which define in DataFrame API, it comes in handy when I want to make use of aggregate operations on DataFrame columns. Aggregate functions operate on a group of rows and calculate a single return value for every group. The count,avg and sum are what I used from aggregate function in my program.\n",
    "\n",
    "Lastly, the desc returns a sort expression based on the descending order of the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1082290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "#importing all types from pyspark.sql.type\n",
    "from pyspark.sql.types import *\n",
    "#importing count, average and sum from pyspark.sql.function\n",
    "from pyspark.sql.functions import count,avg,sum\n",
    "#importing col and desc from pyspark.sql.function\n",
    "from pyspark.sql.functions import col, desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbe69259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://458ad9189293:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>breathest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=breathest>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark =SparkSession\\\n",
    ".builder\\\n",
    ".appName('Python Spark SQL')\\\n",
    ".config('spark.some.config.option','some-value')\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark_context=spark.sparkContext\n",
    "spark_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "541d4609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/notebookuser\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162e4c4",
   "metadata": {},
   "source": [
    "This function parseline returns a key value pair or tuple (Reason, 1).\n",
    "The parseline ignores all the data except the reason and the imposed value of 1 is used to facilitate the count. It is possible to include all the remaining fields in the line as the return value of parseline. The lines variable is used to browse the file directory of the text file named \"DigitalBreathTestData2013.txt\" using spark_context variable that was created earlier with textFile. The Read a texFile from Hadoop Distributed File System ( HDFS ), a local file system (available on all nodes) or any Hadoop-supported file system URI, and return it as an Resilient Distributed Dataset (RDD) of Strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c76d0011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497791"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseline(line):\n",
    "    #The split() method splits a string into a list.\n",
    "    fields = line.split(',')\n",
    "    Reason =fields[0]\n",
    "    return(Reason, 1)\n",
    "lines=spark_context.textFile('DigitalBreathTestData2013.txt')\n",
    "#lines=spark_context.textFile('Users/yusufoyebanji/Downloads/DigitalBreathTestData2013.txt')\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cb2401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The map is used to iterate all the fields. map() function returns a map object\n",
    "#of the results after applying the given function to each item of a given iterable\n",
    "allReasons=lines.map(parseline)\n",
    "\n",
    "#extract header\n",
    "header=lines.first()\n",
    "justdata=lines.filter(lambda row:row !=header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dca13",
   "metadata": {},
   "source": [
    "The parseline2 is used to get all the fields in the \"DigitalBreathTestData2013.txt\" and the return the fields. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0593f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseline2(line):\n",
    "    fields = line.split(',')\n",
    "    Reason = fields[0]\n",
    "    Month = fields [1]\n",
    "    Year = fields [2]\n",
    "    WeekType = fields [3]\n",
    "    TimeBand = fields [4]\n",
    "    BreathAlcoholLevel = int(fields [5])\n",
    "    AgeBand = fields [6]\n",
    "    \n",
    "    if fields [7] == 'Female':\n",
    "        Gender = 2\n",
    "    else:\n",
    "        Gender = 1\n",
    "    return (Reason,Month,Year,WeekType,TimeBand,BreathAlcoholLevel,AgeBand,Gender)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e90f65",
   "metadata": {},
   "source": [
    "PySpark StructType & StructField classes are used to specify the schema to the DataFrame and creating complex columns like nested struct, array and map columns. StructType is a collection of StructField’s that defines column name, column data type, boolean to specify if the field can be nullable or not and metadata. The StringType() Converts an internal SQL object into a native Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47c526d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema2 = StructType(\n",
    "    [StructField('Reason', StringType(),True),\n",
    "     StructField('Month', StringType(),True),\n",
    "     StructField('Year', IntegerType(),True),\n",
    "     StructField('WeekType', StringType(),True),\n",
    "     StructField('TimeBand', StringType(),True),\n",
    "     StructField('AlcoholLevel', IntegerType(),True),\n",
    "     StructField('AgeBand', StringType(),True),\n",
    "     StructField('Gender', StringType(),True)\n",
    "     \n",
    "    \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369de3f",
   "metadata": {},
   "source": [
    "The spark.read.load stored in the data_frame variable loads the text file and format it in Comma Separated Values (CSV). A CSV is a plain text file that contains a list of data. The comma (,) used in sep is used to separate the values and \".withColumnRenamed\" returns a new DataFrame by renaming an existing column. This is a no-op if schema doesn’t contain the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc338dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+--------+--------+------------+-------+------+\n",
      "|              Reason|Month|Year|WeekType|TimeBand|AlcoholLevel|AgeBand|Gender|\n",
      "+--------------------+-----+----+--------+--------+------------+-------+------+\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|12am-4am|          80|  30-39|  Male|\n",
      "|Road Traffic Coll...|  Jan|2013| Weekday|12am-4am|           0|  Other|  Male|\n",
      "|Road Traffic Coll...|  Jan|2013| Weekday|12am-4am|          96|  Other|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|12am-4am|           0|  40-49|Female|\n",
      "|Suspicion of Alcohol|  Jan|2013| Weekday|12am-4am|           0|  40-49|  Male|\n",
      "|Road Traffic Coll...|  Jan|2013| Weekday|8am-12pm|          45|  Other|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|12am-4am|          60|  30-39|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|12am-4am|           0|  16-19|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|12am-4am|           0|  16-19|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|8am-12pm|           0|  50-59|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday|8pm-12pm|           0|  30-39|  Male|\n",
      "|Suspicion of Alcohol|  Jan|2013| Weekday|12am-4am|         110|  30-39|  Male|\n",
      "|Suspicion of Alcohol|  Jan|2013| Weekday|12am-4am|          46|  16-19|  Male|\n",
      "|Road Traffic Coll...|  Jan|2013| Weekday| 4am-8am|           0|  Other|  Male|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday| 4pm-8pm|           0|  25-29|Female|\n",
      "|Moving Traffic Vi...|  Jan|2013| Weekday| 4pm-8pm|           0|  30-39|  Male|\n",
      "|Road Traffic Coll...|  Jan|2013| Weekday|8am-12pm|          33|  Other|  Male|\n",
      "|Road Traffic Coll...|  Jan|2013| Weekday| 4am-8am|          83|  20-24|  Male|\n",
      "|               Other|  Jan|2013| Weekday|8am-12pm|           0|  30-39|  Male|\n",
      "|Suspicion of Alcohol|  Jan|2013| Weekday| 4pm-8pm|           0|  Other|  Male|\n",
      "+--------------------+-----+----+--------+--------+------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame = spark.read.load(\"DigitalBreathTestData2013.txt\",\n",
    "                             format = 'csv', sep = ',', \n",
    "                             schema = myschema2,\n",
    "                             header= 'true')\\\n",
    "                            .withColumnRenamed('BreathAlcoholLevel(microg 100ml)', 'AlcoholLevel')\n",
    "\n",
    "#to display the data frame\n",
    "data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc1c41b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Reason,StringType,true),StructField(Month,StringType,true),StructField(Year,IntegerType,true),StructField(WeekType,StringType,true),StructField(TimeBand,StringType,true),StructField(AlcoholLevel,IntegerType,true),StructField(AgeBand,StringType,true),StructField(Gender,StringType,true)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9a5a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(Reason)|\n",
      "+-------------+\n",
      "|       497790|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The count return the number of times the values of Reason appears \n",
    "data_frame. agg(count('Reason')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9475883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(Reason)|\n",
      "+-------------+\n",
      "|        51488|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This select from the \"Reason\" where the AlcoholLevel is greater than 35 and then show the result after counting.\n",
    "data_frame.select('Reason').where(data_frame.AlcoholLevel > 35).agg(count('Reason')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb4d3efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+---------+\n",
      "|Gender|AgeBand|Month|Aggregate|\n",
      "+------+-------+-----+---------+\n",
      "|  Male|  30-39|  Jun|      980|\n",
      "|  Male|  30-39|  May|      868|\n",
      "|  Male|  30-39|  Dec|      846|\n",
      "|  Male|  Other|  Dec|      823|\n",
      "|  Male|  30-39|  Jul|      805|\n",
      "|  Male|  30-39|  Aug|      800|\n",
      "|  Male|  30-39|  Mar|      782|\n",
      "|  Male|  30-39|  Apr|      767|\n",
      "|  Male|  Other|  Aug|      734|\n",
      "|  Male|  30-39|  Oct|      732|\n",
      "|  Male|  30-39|  Nov|      726|\n",
      "|  Male|  40-49|  Jun|      698|\n",
      "|  Male|  40-49|  May|      681|\n",
      "|  Male|  20-24|  Jun|      653|\n",
      "|  Male|  40-49|  Jul|      643|\n",
      "|  Male|  20-24|  Dec|      639|\n",
      "|  Male|  20-24|  May|      630|\n",
      "|  Male|  20-24|  Mar|      627|\n",
      "|  Male|  30-39|  Sep|      626|\n",
      "|  Male|  25-29|  Jun|      625|\n",
      "+------+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This select ALL where the AlcoholLevel is greater than 35 and then group it by the 'Gender', 'AgeBand', 'Month'\n",
    "#count the values based on the fields displayed. The \"Reason\" is changed to \"Aggregate\" by using the alias. The order \n",
    "#is used to sort the table using the \"Aggregate\" field and descending is set to false to start counting from the\n",
    "#highest. The default is ascending if not set.\n",
    "count_df = (data_frame.select('*')\n",
    "        .where(data_frame.AlcoholLevel>35)\n",
    "        .groupBy('Gender', 'AgeBand', 'Month')\n",
    "        .agg(count(\"Reason\").alias('Aggregate'))\n",
    "        .orderBy('Aggregate', ascending=False)\n",
    "               )\n",
    "\n",
    "count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94c118b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+---------+\n",
      "|Gender|AgeBand|Month|Aggregate|\n",
      "+------+-------+-----+---------+\n",
      "|  Male|  30-39|  Jun|      980|\n",
      "+------+-------+-----+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing one single column which displays the largest target group  \n",
    "count_df.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
